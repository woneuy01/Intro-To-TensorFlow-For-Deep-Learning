{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "l10c03_nlp_constructing_text_generation_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woneuy01/Intro-To-TensorFlow-For-Deep-Learning/blob/master/l10c03_nlp_constructing_text_generation_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "punL79CN7Ox6"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "_ckMIh7O7s6D",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ph5eir3Pf-3z"
      },
      "source": [
        "# Constructing a Text Generation Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S5Uhzt6vVIB2"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7GbGfr_oLCat"
      },
      "source": [
        "Using most of the techniques you've already learned, it's now possible to generate new text by predicting the next word that follows a given seed word. To practice this method, we'll use the [Kaggle Song Lyrics Dataset](https://www.kaggle.com/mousehead/songlyrics)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4aHK2CYygXom"
      },
      "source": [
        "## Import TensorFlow and related functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2LmLTREBf5ng",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Other imports for processing data\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GmLTO_dpgge9"
      },
      "source": [
        "## Get the Dataset\n",
        "\n",
        "As noted above, we'll utilize the [Song Lyrics dataset](https://www.kaggle.com/mousehead/songlyrics) on Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4Bf5FVHfganK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "outputId": "65237ae6-e72c-423d-f48e-85f4f1d4550f"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 \\\n",
        "    -O /tmp/songdata.csv"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-20 04:54:02--  https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.195.102, 74.125.195.138, 74.125.195.100, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.195.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/4e8el70slb2kg8h8sso486q6eia8t0nf/1587358425000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-04-20 04:54:04--  https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/4e8el70slb2kg8h8sso486q6eia8t0nf/1587358425000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)... 74.125.142.132, 2607:f8b0:400e:c08::84\n",
            "Connecting to doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)|74.125.142.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/csv]\n",
            "Saving to: ‘/tmp/songdata.csv’\n",
            "\n",
            "/tmp/songdata.csv       [   <=>              ]  69.08M   140MB/s    in 0.5s    \n",
            "\n",
            "2020-04-20 04:54:05 (140 MB/s) - ‘/tmp/songdata.csv’ saved [72436445]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gu1BTzMIS1oy"
      },
      "source": [
        "## **First 10 Songs**\n",
        "\n",
        "Let's first look at just 10 songs from the dataset, and see how things perform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fmb9rGaAUDO-"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "Let's perform some basic preprocessing to get rid of punctuation and make everything lowercase. We'll then split the lyrics up by line and tokenize the lyrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2AVAvyF_Vuh5",
        "colab": {}
      },
      "source": [
        "def tokenize_corpus(corpus, num_words=-1):\n",
        "  # Fit a Tokenizer on the corpus\n",
        "  if num_words > -1:\n",
        "    tokenizer = Tokenizer(num_words=num_words)\n",
        "  else:\n",
        "    tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  return tokenizer\n",
        "\n",
        "def create_lyrics_corpus(dataset, field):\n",
        "  # Remove all other punctuation\n",
        "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
        "  # Make it lowercase\n",
        "  dataset[field] = dataset[field].str.lower()\n",
        "  # Make it one long string to split by line\n",
        "  lyrics = dataset[field].str.cat()\n",
        "  corpus = lyrics.split('\\n')\n",
        "  # Remove any trailing whitespace\n",
        "  for l in range(len(corpus)):\n",
        "    corpus[l] = corpus[l].rstrip()\n",
        "  # Remove any empty lines\n",
        "  corpus = [l for l in corpus if l != '']\n",
        "\n",
        "  return corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "apcEXp7WhVBs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "96738b0e-40a8-4643-9234-513c2aed2a25"
      },
      "source": [
        "# Read the dataset from csv - just first 10 songs for now\n",
        "dataset = pd.read_csv('/tmp/songdata.csv', dtype=str)[:10]\n",
        "# Create the corpus using the 'text' column containing lyrics\n",
        "corpus = create_lyrics_corpus(dataset, 'text')\n",
        "# Tokenize the corpus\n",
        "tokenizer = tokenize_corpus(corpus)\n",
        "\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(total_words)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'you': 1, 'i': 2, 'and': 3, 'a': 4, 'me': 5, 'the': 6, 'is': 7, 'my': 8, 'to': 9, 'ma': 10, 'it': 11, 'of': 12, 'im': 13, 'your': 14, 'love': 15, 'so': 16, 'as': 17, 'that': 18, 'in': 19, 'andante': 20, 'boomaboomerang': 21, 'make': 22, 'on': 23, 'oh': 24, 'for': 25, 'but': 26, 'new': 27, 'bang': 28, 'its': 29, 'be': 30, 'like': 31, 'know': 32, 'now': 33, 'how': 34, 'could': 35, 'youre': 36, 'sing': 37, 'never': 38, 'no': 39, 'chiquitita': 40, 'can': 41, 'we': 42, 'song': 43, 'had': 44, 'good': 45, 'youll': 46, 'she': 47, 'just': 48, 'girl': 49, 'again': 50, 'will': 51, 'take': 52, 'please': 53, 'let': 54, 'am': 55, 'eyes': 56, 'was': 57, 'always': 58, 'cassandra': 59, 'blue': 60, 'time': 61, 'dont': 62, 'were': 63, 'return': 64, 'once': 65, 'then': 66, 'sorry': 67, 'cryin': 68, 'over': 69, 'feel': 70, 'ever': 71, 'believe': 72, 'what': 73, 'do': 74, 'go': 75, 'all': 76, 'out': 77, 'think': 78, 'every': 79, 'leave': 80, 'look': 81, 'at': 82, 'way': 83, 'one': 84, 'music': 85, 'down': 86, 'our': 87, 'give': 88, 'learn': 89, 'more': 90, 'us': 91, 'would': 92, 'there': 93, 'before': 94, 'when': 95, 'with': 96, 'feeling': 97, 'play': 98, 'cause': 99, 'away': 100, 'here': 101, 'have': 102, 'yes': 103, 'baby': 104, 'get': 105, 'didnt': 106, 'see': 107, 'did': 108, 'closed': 109, 'realized': 110, 'crazy': 111, 'world': 112, 'lord': 113, 'shes': 114, 'kind': 115, 'without': 116, 'if': 117, 'touch': 118, 'strong': 119, 'making': 120, 'such': 121, 'found': 122, 'true': 123, 'stay': 124, 'together': 125, 'thought': 126, 'come': 127, 'they': 128, 'sweet': 129, 'tender': 130, 'sender': 131, 'tune': 132, 'humdehumhum': 133, 'gonna': 134, 'last': 135, 'leaving': 136, 'sleep': 137, 'only': 138, 'saw': 139, 'tell': 140, 'hes': 141, 'her': 142, 'sound': 143, 'tread': 144, 'lightly': 145, 'ground': 146, 'ill': 147, 'show': 148, 'life': 149, 'too': 150, 'used': 151, 'darling': 152, 'meant': 153, 'break': 154, 'end': 155, 'yourself': 156, 'little': 157, 'dumbedumdum': 158, 'bedumbedumdum': 159, 'youve': 160, 'dumbbedumbdumb': 161, 'bedumbbedumbdumb': 162, 'by': 163, 'theyre': 164, 'alone': 165, 'misunderstood': 166, 'day': 167, 'dawning': 168, 'some': 169, 'wanted': 170, 'none': 171, 'listen': 172, 'words': 173, 'warning': 174, 'darkest': 175, 'nights': 176, 'nobody': 177, 'knew': 178, 'fight': 179, 'caught': 180, 'really': 181, 'power': 182, 'dreams': 183, 'weave': 184, 'until': 185, 'final': 186, 'hour': 187, 'morning': 188, 'ship': 189, 'gone': 190, 'grieving': 191, 'still': 192, 'pain': 193, 'cry': 194, 'sun': 195, 'try': 196, 'face': 197, 'something': 198, 'sees': 199, 'makes': 200, 'fine': 201, 'who': 202, 'mine': 203, 'leaves': 204, 'walk': 205, 'hand': 206, 'well': 207, 'about': 208, 'things': 209, 'slow': 210, 'theres': 211, 'talk': 212, 'why': 213, 'up': 214, 'lousy': 215, 'packing': 216, 'ive': 217, 'gotta': 218, 'near': 219, 'keeping': 220, 'intention': 221, 'growing': 222, 'taking': 223, 'dimension': 224, 'even': 225, 'better': 226, 'thank': 227, 'god': 228, 'not': 229, 'somebody': 230, 'happy': 231, 'question': 232, 'smile': 233, 'mean': 234, 'much': 235, 'kisses': 236, 'around': 237, 'anywhere': 238, 'advice': 239, 'care': 240, 'use': 241, 'selfish': 242, 'tool': 243, 'fool': 244, 'showing': 245, 'boomerang': 246, 'throwing': 247, 'warm': 248, 'kiss': 249, 'surrender': 250, 'giving': 251, 'been': 252, 'door': 253, 'burning': 254, 'bridges': 255, 'being': 256, 'moving': 257, 'though': 258, 'behind': 259, 'are': 260, 'must': 261, 'sure': 262, 'stood': 263, 'hope': 264, 'this': 265, 'deny': 266, 'sad': 267, 'quiet': 268, 'truth': 269, 'heartaches': 270, 'scars': 271, 'dancing': 272, 'sky': 273, 'shining': 274, 'above': 275, 'hear': 276, 'came': 277, 'couldnt': 278, 'everything': 279, 'back': 280, 'long': 281, 'waitin': 282, 'cold': 283, 'chills': 284, 'bone': 285, 'youd': 286, 'wonderful': 287, 'means': 288, 'special': 289, 'smiles': 290, 'lucky': 291, 'fellow': 292, 'park': 293, 'holds': 294, 'squeezes': 295, 'walking': 296, 'hours': 297, 'talking': 298, 'plan': 299, 'easy': 300, 'gently': 301, 'summer': 302, 'evening': 303, 'breeze': 304, 'grow': 305, 'fingers': 306, 'soft': 307, 'light': 308, 'body': 309, 'velvet': 310, 'night': 311, 'soul': 312, 'slowly': 313, 'shimmer': 314, 'thousand': 315, 'butterflies': 316, 'float': 317, 'put': 318, 'rotten': 319, 'boy': 320, 'tough': 321, 'stuff': 322, 'saying': 323, 'need': 324, 'anymore': 325, 'enough': 326, 'standing': 327, 'creep': 328, 'felt': 329, 'cheap': 330, 'notion': 331, 'deep': 332, 'dumb': 333, 'mistake': 334, 'entitled': 335, 'another': 336, 'beg': 337, 'forgive': 338, 'an': 339, 'feels': 340, 'hoot': 341, 'holler': 342, 'mad': 343, 'under': 344, 'heel': 345, 'holy': 346, 'christ': 347, 'deal': 348, 'sick': 349, 'tired': 350, 'tedious': 351, 'ways': 352, 'aint': 353, 'walkin': 354, 'cutting': 355, 'tie': 356, 'wanna': 357, 'into': 358, 'eye': 359, 'myself': 360, 'counting': 361, 'pride': 362, 'unright': 363, 'neighbours': 364, 'ride': 365, 'burying': 366, 'past': 367, 'peace': 368, 'free': 369, 'sucker': 370, 'street': 371, 'singing': 372, 'shouting': 373, 'staying': 374, 'alive': 375, 'city': 376, 'dead': 377, 'hiding': 378, 'their': 379, 'shame': 380, 'hollow': 381, 'laughter': 382, 'while': 383, 'crying': 384, 'bed': 385, 'pity': 386, 'believed': 387, 'lost': 388, 'from': 389, 'start': 390, 'suffer': 391, 'sell': 392, 'secrets': 393, 'bargain': 394, 'playing': 395, 'smart': 396, 'aching': 397, 'hearts': 398, 'sailing': 399, 'father': 400, 'sister': 401, 'reason': 402, 'linger': 403, 'deeply': 404, 'future': 405, 'casting': 406, 'shadow': 407, 'else': 408, 'fate': 409, 'bags': 410, 'thorough': 411, 'knowing': 412, 'late': 413, 'wait': 414, 'watched': 415, 'harbor': 416, 'sunrise': 417, 'sails': 418, 'almost': 419, 'slack': 420, 'cool': 421, 'rain': 422, 'deck': 423, 'tiny': 424, 'figure': 425, 'rigid': 426, 'restrained': 427, 'filled': 428, 'whats': 429, 'wrong': 430, 'enchained': 431, 'own': 432, 'sorrow': 433, 'tomorrow': 434, 'hate': 435, 'shoulder': 436, 'best': 437, 'friend': 438, 'rely': 439, 'broken': 440, 'feather': 441, 'patch': 442, 'walls': 443, 'tumbling': 444, 'loves': 445, 'blown': 446, 'candle': 447, 'seems': 448, 'hard': 449, 'handle': 450, 'id': 451, 'thinking': 452, 'went': 453, 'house': 454, 'hardly': 455, 'guy': 456, 'closing': 457, 'front': 458, 'emptiness': 459, 'he': 460, 'disapeared': 461, 'his': 462, 'car': 463, 'stunned': 464, 'dreamed': 465, 'lifes': 466, 'part': 467, 'move': 468, 'feet': 469, 'pavement': 470, 'acted': 471, 'told': 472, 'lies': 473, 'meet': 474, 'other': 475, 'guys': 476, 'stupid': 477, 'blind': 478, 'smiled': 479, 'took': 480, 'said': 481, 'may': 482, 'couple': 483, 'men': 484, 'them': 485, 'brother': 486, 'joe': 487, 'seeing': 488, 'lot': 489, 'him': 490, 'nice': 491, 'sitting': 492, 'sittin': 493, 'memories': 494}\n",
            "495\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v9x68iN_X6FK"
      },
      "source": [
        "### Create Sequences and Labels\n",
        "\n",
        "After preprocessing, we next need to create sequences and labels. Creating the sequences themselves is similar to before with `texts_to_sequences`, but also including the use of [N-Grams](https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9); creating the labels will now utilize those sequences as well as utilize one-hot encoding over all potential output words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QmlTsUqfikVO",
        "colab": {}
      },
      "source": [
        "sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tsequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences for equal input length \n",
        "max_sequence_len = max([len(seq) for seq in sequences])\n",
        "sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# Split sequences between the \"input\" sequence and \"output\" predicted word\n",
        "input_sequences, labels = sequences[:,:-1], sequences[:,-1]\n",
        "# One-hot encode the labels\n",
        "one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zsmu3aEId49i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 823
        },
        "outputId": "d2de279b-b83a-4233-9f0a-95c1892be7b8"
      },
      "source": [
        "# Check out how some of our data is being stored\n",
        "# The Tokenizer has just a single index per word\n",
        "print(tokenizer.word_index['know'])\n",
        "print(tokenizer.word_index['feeling'])\n",
        "# Input sequences will have multiple indexes\n",
        "print(input_sequences[5])\n",
        "print(input_sequences[6])\n",
        "# And the one hot labels will be as long as the full spread of tokenized words\n",
        "print(one_hot_labels[5])\n",
        "print(one_hot_labels[6])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32\n",
            "97\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29\n",
            "   4]\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29   4\n",
            " 287]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-1TAJMlmfO8r"
      },
      "source": [
        "### Train a Text Generation Model\n",
        "\n",
        "Building an RNN to train our text generation model will be very similar to the sentiment models you've built previously. The only real change necessary is to make sure to use Categorical instead of Binary Cross Entropy as the loss function - we could use Binary before since the sentiment was only 0 or 1, but now there are hundreds of categories.\n",
        "\n",
        "From there, we should also consider using *more* epochs than before, as text generation can take a little longer to converge than sentiment analysis, *and* we aren't working with all that much data yet. I'll set it at 200 epochs here since we're only use part of the dataset, and training will tail off quite a bit over that many epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G1YXuxIqfygN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7aa3ae1b-fce0-45d1-f712-7f9132a20d70"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(20)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(input_sequences, one_hot_labels, epochs=200, verbose=1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 5.9965 - accuracy: 0.0288\n",
            "Epoch 2/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 5.4395 - accuracy: 0.0383\n",
            "Epoch 3/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 5.3682 - accuracy: 0.0399\n",
            "Epoch 4/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 5.3203 - accuracy: 0.0353\n",
            "Epoch 5/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 5.2508 - accuracy: 0.0429\n",
            "Epoch 6/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 5.1773 - accuracy: 0.0429\n",
            "Epoch 7/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 5.0965 - accuracy: 0.0585\n",
            "Epoch 8/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.0055 - accuracy: 0.0530\n",
            "Epoch 9/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 4.9084 - accuracy: 0.0691\n",
            "Epoch 10/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 4.8213 - accuracy: 0.0903\n",
            "Epoch 11/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 4.7289 - accuracy: 0.0858\n",
            "Epoch 12/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 4.6430 - accuracy: 0.0858\n",
            "Epoch 13/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 4.5656 - accuracy: 0.0943\n",
            "Epoch 14/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 4.4812 - accuracy: 0.0969\n",
            "Epoch 15/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 4.4002 - accuracy: 0.1070\n",
            "Epoch 16/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 4.3302 - accuracy: 0.1075\n",
            "Epoch 17/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 4.2544 - accuracy: 0.1150\n",
            "Epoch 18/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 4.1774 - accuracy: 0.1352\n",
            "Epoch 19/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 4.1291 - accuracy: 0.1453\n",
            "Epoch 20/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 4.0445 - accuracy: 0.1574\n",
            "Epoch 21/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 3.9686 - accuracy: 0.1786\n",
            "Epoch 22/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 3.9040 - accuracy: 0.1872\n",
            "Epoch 23/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 3.8340 - accuracy: 0.1973\n",
            "Epoch 24/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 3.7903 - accuracy: 0.2124\n",
            "Epoch 25/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 3.7125 - accuracy: 0.2316\n",
            "Epoch 26/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 3.6462 - accuracy: 0.2432\n",
            "Epoch 27/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.5714 - accuracy: 0.2593\n",
            "Epoch 28/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 3.5087 - accuracy: 0.2851\n",
            "Epoch 29/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 3.4487 - accuracy: 0.3012\n",
            "Epoch 30/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.3779 - accuracy: 0.3254\n",
            "Epoch 31/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 3.3113 - accuracy: 0.3441\n",
            "Epoch 32/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 3.2463 - accuracy: 0.3466\n",
            "Epoch 33/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.2154 - accuracy: 0.3557\n",
            "Epoch 34/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 3.1381 - accuracy: 0.3724\n",
            "Epoch 35/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 3.0822 - accuracy: 0.3784\n",
            "Epoch 36/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 3.0211 - accuracy: 0.3956\n",
            "Epoch 37/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.9920 - accuracy: 0.3981\n",
            "Epoch 38/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.9263 - accuracy: 0.4162\n",
            "Epoch 39/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.8705 - accuracy: 0.4334\n",
            "Epoch 40/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.8012 - accuracy: 0.4470\n",
            "Epoch 41/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.7441 - accuracy: 0.4475\n",
            "Epoch 42/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.6926 - accuracy: 0.4632\n",
            "Epoch 43/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.6270 - accuracy: 0.4753\n",
            "Epoch 44/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.5846 - accuracy: 0.4904\n",
            "Epoch 45/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 2.5723 - accuracy: 0.4879\n",
            "Epoch 46/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.5236 - accuracy: 0.4869\n",
            "Epoch 47/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 2.4699 - accuracy: 0.5045\n",
            "Epoch 48/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.4168 - accuracy: 0.5121\n",
            "Epoch 49/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.3750 - accuracy: 0.5197\n",
            "Epoch 50/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.3336 - accuracy: 0.5353\n",
            "Epoch 51/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.2941 - accuracy: 0.5394\n",
            "Epoch 52/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.2621 - accuracy: 0.5424\n",
            "Epoch 53/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.2158 - accuracy: 0.5505\n",
            "Epoch 54/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.1743 - accuracy: 0.5560\n",
            "Epoch 55/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.1558 - accuracy: 0.5525\n",
            "Epoch 56/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.1141 - accuracy: 0.5661\n",
            "Epoch 57/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.0766 - accuracy: 0.5691\n",
            "Epoch 58/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.0542 - accuracy: 0.5782\n",
            "Epoch 59/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.0104 - accuracy: 0.5873\n",
            "Epoch 60/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.9601 - accuracy: 0.5954\n",
            "Epoch 61/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.9263 - accuracy: 0.6004\n",
            "Epoch 62/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.8927 - accuracy: 0.6145\n",
            "Epoch 63/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.8610 - accuracy: 0.6105\n",
            "Epoch 64/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.8352 - accuracy: 0.6216\n",
            "Epoch 65/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.7990 - accuracy: 0.6367\n",
            "Epoch 66/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.7654 - accuracy: 0.6398\n",
            "Epoch 67/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.7349 - accuracy: 0.6448\n",
            "Epoch 68/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.7018 - accuracy: 0.6539\n",
            "Epoch 69/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.6808 - accuracy: 0.6544\n",
            "Epoch 70/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.6487 - accuracy: 0.6660\n",
            "Epoch 71/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.6211 - accuracy: 0.6690\n",
            "Epoch 72/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5998 - accuracy: 0.6736\n",
            "Epoch 73/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.5823 - accuracy: 0.6771\n",
            "Epoch 74/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.5483 - accuracy: 0.6842\n",
            "Epoch 75/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.5246 - accuracy: 0.6877\n",
            "Epoch 76/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.5082 - accuracy: 0.6927\n",
            "Epoch 77/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.4796 - accuracy: 0.7023\n",
            "Epoch 78/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4499 - accuracy: 0.7008\n",
            "Epoch 79/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.4329 - accuracy: 0.7043\n",
            "Epoch 80/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.4101 - accuracy: 0.7124\n",
            "Epoch 81/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.3865 - accuracy: 0.7139\n",
            "Epoch 82/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.3734 - accuracy: 0.7180\n",
            "Epoch 83/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.3538 - accuracy: 0.7220\n",
            "Epoch 84/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.3317 - accuracy: 0.7205\n",
            "Epoch 85/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.3022 - accuracy: 0.7281\n",
            "Epoch 86/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.2779 - accuracy: 0.7427\n",
            "Epoch 87/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.2767 - accuracy: 0.7392\n",
            "Epoch 88/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.2927 - accuracy: 0.7351\n",
            "Epoch 89/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.2526 - accuracy: 0.7386\n",
            "Epoch 90/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.2205 - accuracy: 0.7482\n",
            "Epoch 91/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.2051 - accuracy: 0.7497\n",
            "Epoch 92/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.1882 - accuracy: 0.7523\n",
            "Epoch 93/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.1623 - accuracy: 0.7558\n",
            "Epoch 94/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.1326 - accuracy: 0.7679\n",
            "Epoch 95/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.1137 - accuracy: 0.7689\n",
            "Epoch 96/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.1037 - accuracy: 0.7699\n",
            "Epoch 97/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.0924 - accuracy: 0.7750\n",
            "Epoch 98/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.0710 - accuracy: 0.7780\n",
            "Epoch 99/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.0545 - accuracy: 0.7780\n",
            "Epoch 100/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.0456 - accuracy: 0.7735\n",
            "Epoch 101/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.0391 - accuracy: 0.7856\n",
            "Epoch 102/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.0604 - accuracy: 0.7785\n",
            "Epoch 103/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.0309 - accuracy: 0.7820\n",
            "Epoch 104/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.0071 - accuracy: 0.7871\n",
            "Epoch 105/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.9889 - accuracy: 0.7896\n",
            "Epoch 106/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.9791 - accuracy: 0.7926\n",
            "Epoch 107/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.9702 - accuracy: 0.7926\n",
            "Epoch 108/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9571 - accuracy: 0.7997\n",
            "Epoch 109/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.9527 - accuracy: 0.7901\n",
            "Epoch 110/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9461 - accuracy: 0.7891\n",
            "Epoch 111/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.9164 - accuracy: 0.8017\n",
            "Epoch 112/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.8968 - accuracy: 0.8047\n",
            "Epoch 113/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.8868 - accuracy: 0.8083\n",
            "Epoch 114/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.8796 - accuracy: 0.8093\n",
            "Epoch 115/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.8716 - accuracy: 0.8093\n",
            "Epoch 116/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.8658 - accuracy: 0.8073\n",
            "Epoch 117/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8487 - accuracy: 0.8103\n",
            "Epoch 118/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.8306 - accuracy: 0.8189\n",
            "Epoch 119/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8280 - accuracy: 0.8184\n",
            "Epoch 120/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.8212 - accuracy: 0.8169\n",
            "Epoch 121/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.8438 - accuracy: 0.8042\n",
            "Epoch 122/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.8189 - accuracy: 0.8153\n",
            "Epoch 123/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.8017 - accuracy: 0.8189\n",
            "Epoch 124/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.7885 - accuracy: 0.8209\n",
            "Epoch 125/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.7744 - accuracy: 0.8234\n",
            "Epoch 126/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.7595 - accuracy: 0.8269\n",
            "Epoch 127/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.7503 - accuracy: 0.8325\n",
            "Epoch 128/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.7443 - accuracy: 0.8269\n",
            "Epoch 129/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.7360 - accuracy: 0.8305\n",
            "Epoch 130/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7442 - accuracy: 0.8315\n",
            "Epoch 131/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.7297 - accuracy: 0.8325\n",
            "Epoch 132/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.7232 - accuracy: 0.8350\n",
            "Epoch 133/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.7124 - accuracy: 0.8375\n",
            "Epoch 134/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7013 - accuracy: 0.8380\n",
            "Epoch 135/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.7050 - accuracy: 0.8385\n",
            "Epoch 136/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.7284 - accuracy: 0.8280\n",
            "Epoch 137/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.7034 - accuracy: 0.8360\n",
            "Epoch 138/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6818 - accuracy: 0.8406\n",
            "Epoch 139/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6834 - accuracy: 0.8431\n",
            "Epoch 140/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6724 - accuracy: 0.8431\n",
            "Epoch 141/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6639 - accuracy: 0.8426\n",
            "Epoch 142/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6497 - accuracy: 0.8446\n",
            "Epoch 143/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6650 - accuracy: 0.8461\n",
            "Epoch 144/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6499 - accuracy: 0.8426\n",
            "Epoch 145/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6313 - accuracy: 0.8466\n",
            "Epoch 146/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6200 - accuracy: 0.8502\n",
            "Epoch 147/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6102 - accuracy: 0.8572\n",
            "Epoch 148/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6020 - accuracy: 0.8532\n",
            "Epoch 149/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5924 - accuracy: 0.8572\n",
            "Epoch 150/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5866 - accuracy: 0.8527\n",
            "Epoch 151/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5798 - accuracy: 0.8628\n",
            "Epoch 152/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5748 - accuracy: 0.8597\n",
            "Epoch 153/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5926 - accuracy: 0.8572\n",
            "Epoch 154/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.6236 - accuracy: 0.8461\n",
            "Epoch 155/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6029 - accuracy: 0.8527\n",
            "Epoch 156/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5851 - accuracy: 0.8557\n",
            "Epoch 157/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5661 - accuracy: 0.8653\n",
            "Epoch 158/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5561 - accuracy: 0.8673\n",
            "Epoch 159/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5488 - accuracy: 0.8668\n",
            "Epoch 160/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5371 - accuracy: 0.8678\n",
            "Epoch 161/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5281 - accuracy: 0.8744\n",
            "Epoch 162/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5234 - accuracy: 0.8739\n",
            "Epoch 163/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5155 - accuracy: 0.8769\n",
            "Epoch 164/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.5110 - accuracy: 0.8754\n",
            "Epoch 165/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5043 - accuracy: 0.8794\n",
            "Epoch 166/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4997 - accuracy: 0.8814\n",
            "Epoch 167/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.4956 - accuracy: 0.8749\n",
            "Epoch 168/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4913 - accuracy: 0.8814\n",
            "Epoch 169/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4901 - accuracy: 0.8804\n",
            "Epoch 170/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4837 - accuracy: 0.8809\n",
            "Epoch 171/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4819 - accuracy: 0.8809\n",
            "Epoch 172/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4860 - accuracy: 0.8850\n",
            "Epoch 173/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4766 - accuracy: 0.8819\n",
            "Epoch 174/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4688 - accuracy: 0.8809\n",
            "Epoch 175/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4645 - accuracy: 0.8835\n",
            "Epoch 176/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6698 - accuracy: 0.8300\n",
            "Epoch 177/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5859 - accuracy: 0.8517\n",
            "Epoch 178/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5353 - accuracy: 0.8653\n",
            "Epoch 179/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4946 - accuracy: 0.8789\n",
            "Epoch 180/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4859 - accuracy: 0.8754\n",
            "Epoch 181/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5181 - accuracy: 0.8658\n",
            "Epoch 182/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4818 - accuracy: 0.8764\n",
            "Epoch 183/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4628 - accuracy: 0.8824\n",
            "Epoch 184/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4483 - accuracy: 0.8829\n",
            "Epoch 185/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4410 - accuracy: 0.8850\n",
            "Epoch 186/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4347 - accuracy: 0.8900\n",
            "Epoch 187/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4304 - accuracy: 0.8890\n",
            "Epoch 188/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4241 - accuracy: 0.8855\n",
            "Epoch 189/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4210 - accuracy: 0.8935\n",
            "Epoch 190/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4176 - accuracy: 0.8920\n",
            "Epoch 191/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4122 - accuracy: 0.8890\n",
            "Epoch 192/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4090 - accuracy: 0.8910\n",
            "Epoch 193/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4060 - accuracy: 0.8905\n",
            "Epoch 194/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4033 - accuracy: 0.8925\n",
            "Epoch 195/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4008 - accuracy: 0.8940\n",
            "Epoch 196/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.3977 - accuracy: 0.8940\n",
            "Epoch 197/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.3930 - accuracy: 0.8935\n",
            "Epoch 198/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.3912 - accuracy: 0.8951\n",
            "Epoch 199/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.3880 - accuracy: 0.8925\n",
            "Epoch 200/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.3849 - accuracy: 0.8946\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AXVFpoREhV6Y"
      },
      "source": [
        "### View the Training Graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aeSNfS7uhch0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "ceebb538-5830-4ef9-e96a-26e1ddd06b1b"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, 'accuracy')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deXhU9d3+8fcn+wYhIWFLAgFZlE2WiNS61a2474pbN6vd1C6PtrY+ttY+/dVuVm2pFmvVautuK7W4VEVRBCXsIHsgJCFAIBuE7PP9/TEDDTGBATJzJpn7dV25mDlzZnJzMpk7Z/sec84hIiLRK8brACIi4i0VgYhIlFMRiIhEORWBiEiUUxGIiES5OK8DHK6srCyXn5/vdQwRkW5l0aJFO51z2R091u2KID8/n8LCQq9jiIh0K2ZW3Nlj2jQkIhLlVAQiIlFORSAiEuVUBCIiUU5FICIS5VQEIiJRTkUgIhLlut15BCIi3U3N3mbiYo3UxI4/cp1zrN+xh/KaBgzonRxP9d4m1m3fTUF+JnkZKcxZu4Mp+ZnkZ6V2eT4VgYhIF6qsa2JRcRW5GcmM7N+LLZV7ufSP86itb2ZcTjrXnTiExPgYlpXUkJYYy5bKvczbuIuK3Y2HfO27zjuOm04d1uWZVQQiErWq6ppIjI8hJeHoPwqdc8zbsIvvPr90/4f6iH5pNLf6MOAbpx/D26t38P2XlgOQFB9DY4uPvqmJfHZ4Xz57TBbH9EvF56C2vpnk+FiG90vj3XUVVOxu5PRR2Ywe2Puoc3ZERSAiPc6O3Q0sL6nhxGGZ9EqK5/31FTS3+shOS6K2oZnmVh8Liip57IMikuNjuaogj9GDerO8tIY122qZ+YUCeifFH/L77Gls4YH/rGPzrjrWbNtNaVU9w7JS+fUV49lR28gjczdSVl3P0zeeyInD+nL7OaNYuLmK2BhjQl4fDDADM+v0e1xVkNeFS6ZjKgIR6fbWbKtle20j/Xsncv+b63jzk+0AjBnUm88Oz2Lm3KIOn3fZpBzqm1r5y7xN+NpctXdVWS2fOabv/vvbahq448VlnDO6P1efMJiEuBh27mnky48v5JPyWkb178Xogb351ueGc9Hxg/bvC7hsUg7V9c1kpSUC/g/8KUMzQ7QUjpyKQES6HZ/P8cBb61iwqZKWVh+Lt1TvfywlIZZbzxhOXmYK//vPlazaWsvVBXlcdUIuO/c0kZ4cT0JcDJkpCft3vDY0t1JatZe9Ta1c9Id5lFbtBf5bBDPmbOD99Tt5f/1OXl5SxrM3T+WbTy9m/Y7dPPqFyZxxbP8Oc8bFxuwvgUimIhCRiNfqcxTvqiMtMY7ahhZmzt3I84WljMtJxwx+MO1YxuWkU7RzD2cd159BfZIBGJaVyvLSGr50Uj4xMZ1vfkmKj2V4v140tfgwg9Kq+v2Pbatp4LmFJVwzJY/JQzK5/YVlXPXIfJaV1vCry8d3WgLdiYpARDxT29BMakIcsZ18SDvnuP8/63h6QTFVe5sPeOy2M4bz3bNHHrB9/eQRWQfMU5CfSUF+8JtiEuJiGNA76YAieOS9jfic45un+9cyPiraxQuLSpk6LJMrC3KDfu1IpiIQEU9srNjDhb//gL5pCVx34hCuKsgjMzXhgHleWbqV37+zgbOO68c5owfQ1OojOT6WYwf2Ysyg9JDkys1IpqRqLwA7aht45uMtXDYph7zMFAB+ctEYMtMSuP7EIQfdydudqAhEJKTWb9/N919azrQxAxjSN5WH39vIeWMH8MaqbcTHxpDTJ5n7XlvD/W+u477Lx3HZJP9f2YuKq7j7lZVMHpLBI9dPJi42PAMh5Gak8PGmSgBmzi2iudXHN08fvv/xtMQ4fnjucWHJEi4qAhEJmc0767juzx9RU9/MksAO3czUBH7x2hoAHrh6ApdMzGHd9t388OUV/GTWKiYOzuAns1Yxd10FfVMT+N1VE8JWAgB5Gcm8srSe7bUNPP1RMZdMyAnJ2byRREUgIl2ivKaeDTv2MGVoJj6f/6/pR97bSFJ8DP+69WTKqurZWlPPVQV5zF5RTmlVPRdPGATAyP69+NUV45n2wFymPTCXVp/jR+cdy7UnDiGtk2EZQiU3IwWfgz+/X0RDs4+vnXZMWL+/F1QEIhK0PY0tzF5RzvyNu2j1OVISYjGDjzZVUlRRB0BOn2R8zlFe08C5Ywfwo/OOIy8zhZH9e+1/nYsn5HzqtY/JTuMbpw/nkXc38sfrJnHOmAFh+3+1lZvhP+Lo2YUlDMtKZdSAXod4RvenIhCR/dZsq6VwcxVVdU1U7m0iLyOFL382HzPj38vLufuVlVTWNdGvVyIpCbHsbWqlscXHpMF9uHbKYAamJ/P4vE2YwYPTJx72yVPfPWsEN50ylF5BnNUbKrkZ/p3CuxtauHbKYM9yhJOKQCRKVNU1AZDR5sic5lYfJZV7mbuughcXl7KyrHb/Y8nxsdQ3t5KZmkDxrr387q11HJ+bzswbJjN5SEanR8ycP37gEWc0M09LAGBAehIxBj4HZ4/u/ucIBENFIBIFFm6u5GtPLcI5x32Xj2f+xl3MXVfBlsq9tATGVhib05t7LhzN2WMGkJ2WSGyMccUjH3LXP1ZQ19TKZZNy+NXl48O649YL+84laGzxMXFwhtdxwkJFINKD7dzTyCPvbuTJ+ZvJy0gBg689tYjYGONzo/oxbewAhmWnMS4nvcNt4b+4bBwXPPQBBUMy+MVl43p8CexzZUEevZI6P9GtpzHn3KHniiAFBQWusLDQ6xgintq5p5G0xDiS4mM/9VhtQzNvrNzGq8vLmbdhJz7nuGxSLnefPxoMnv14C+eMGcDQIA+JLKrYw8D0ZJITPv29pPsws0XOuYKOHtMagUg34pzjR/9YyTMfbwFgcGYKZx3Xn7vOP44Wn49H3i3i0feL2NPYQm5GMjeeMpQrJ+cxvF/a/tc43MMhh2WnHXom6dZUBCLdyLMLS3jm4y1cPimX/L4pLN5SxV/mbWLMoN6s3FrD4/M28/kx/fn6acf4x7vvIUMgSGiFtAjMbBrwIBAL/Nk5d1+7xwcDTwJ9AvPc6ZybHcpMIpHMOcf8jbv46/xi1m7fza49jcTEGMfn9iE2xnhnzQ5OHZnNr64YT2yM4fM5Lp4xj1+8tobKukauPXEw/+/ScV7/N6SbCVkRmFksMAM4GygFFprZLOfcJ21m+1/geefcw2Y2GpgN5Icqk0gkm7uuggffXs+i4iqyeyVyQn4G/Xpl09jiY/7GnexpbOG2M0dw86nD9u/EjIkxvj9tFDc89jGZqQl8//OjPP5fSHcUyjWCKcAG51wRgJk9C1wMtC0CB+y7CGc6sDWEeUQigs/nePDt9QxIT+KawAlLc9bs4MtPLGRQehI/u3gMVxbkdbgjuCMnD8/ie2ePZNLgDPqkJBz6CSLthLIIcoCSNvdLgRPbzXMP8KaZ3QqkAmd19EJmdjNwM8DgwdFxpp/0PLsbmqmsa+LxeZt54sPNxBiM7J/GhLwM7nttDfl9U3j9O6cGXQD7mBm3nTkiRKklGni9s/ga4Ann3G/N7DPAU2Y21jnnazuTc24mMBP8h496kFPkiDjnWFZaw1Pzi/nX8q00tfjf2jdMHcKctTv4znNLOX1kP9Zu381D10w87BIQ6QqhLIIyIK/N/dzAtLZuBKYBOOfmm1kSkAXsCGEukZAoqdzLx5squWxSDmZGSeVevvf8UhZuriI1IZarC/KYkNeHjNR4PjeqH4uKB3HTXwt5akEx43LSuWDckQ/NIHI0QlkEC4ERZjYUfwFMB65tN88W4EzgCTM7DkgCKkKYSSQknHN8+9klLN5SzbrtuxnSN5X7XluNA+65cDSXT8791Bg6BfmZLL77bHbs9p8cdrBr6oqEUsiKwDnXYma3AG/gPzT0L865VWZ2L1DonJsF/A/wqJl9F/+O4y+57naqs0Sd5lYfs1eU89zCEnY3tJCXmUzBkEwWb6lmbE5v/jS3CIAT8jP47ZUTGNw3pdPXMjP6904KV3SRDmmICZHDULyrjm/9fTEry2oZlpXKkL7+yxrWNbUyqn8vXr3tZJ78cDNDs1I549h+OqFLIoaGmBA5Qk0tPp74cBMrymrZtHMP67bvITk+lj9cO5Hzxg4kJsa/L+CBt9Zz/dTBxMfG8NVThnkdW+SwqAhEDuIv8zZx32tryMtMZlhWGjdM7cuXP5u//+IlAHmZKfz2quM9TClydFQEIp2orGtixjsbOPPYfjz2pRO8jiMSMtExuLjIYdq5p5EfvrycuqYW7jz3WK/jiISU1ghEAnY3NLN4SzWzl5fzr+VbaWzx8b2zRzKif8+/eLlENxWBCPD6ym3c+sximlsdKQmxXDB+IF8/7RiNxS9RQUUgUWV5aTVPzNvMV08ZxuhB/vEOt9U08IOXljNqQC9+MO1YJg/JICVBvxoSPfRul6jyf/9ezcebKvnH0jKumpzHRRMG8avX19DU4uOh6RO1BiBRSUUgUWNpSTUfb6rk22eOoK6xhSfnb+a5whL6pMTz26uOVwlI1FIRSNSYOXcjvZPiuOnUYaQlxnH91CF8vLmSaWMH0LvdOEAi0URFIFHhhcISZq/Yxq1nDCct0f+2z89KJT8r1eNkIt5TEUiP1Opz/O2jYh58az1pSXGUVdVz8vAsbj1DF3ARaU9FID2Kz+coLK7ip/9axaqttUwdlkl6cjzjctL5xWXjSIjTOZQi7akIpMd4dG4Rv39nPbUNLQzoncQfrp3I+eMGagRQkUNQEUiPMGvZVn4+ezWnjszmwvEDOXfcwP37AkTk4PSbIt1SQ3MrT364mdyMFFaX1zLz/SKm5Gfy5y8UaPOPyGFSEUi39PN/r+apBcX7718wfiD3XjxWJSByBFQE0u3MWbODpxYU86WT8rlkYg5piXEM76eTwUSOlIpAug3nHH//eAs//dcnjOrfizvPPZak+FivY4l0eyoC6RYWb6niZ69+wpIt1Zw6MpvfXXW8SkCki6gIJOJ9uHEnX3p8IZkpCfzisnFcXZBHTIwOCRXpKioCiVgtrT5eWFTKz179hPy+KTx382fISE3wOpZIj6MikIi0vLSa219Yxrrte5g0uA+PXD9ZJSASIioCiSgbK/bwyLsbeXlJGdlpiTxy/WQ+P6a/zg4WCSEVgUSMHbsbuOQP82jxOW6YOoTvnj2S9GQNDy0SaioCiRi/eWMtDS2tvP6dUzlGF4kRCRsVgXhu9opyFhVX8cKiUm46ZZhKQCTMVATiqVeXb+WWvy8hITaGCXl9uOWM4V5HEok6KgLxzKaddfzwpRVMHNyH57/2GeJjNU6QiBdUBBJ2zjneWr2DO15cRkyM8dD0iSoBEQ+pCCSsVm2t4a5/rGRpSTWjB/ZmxnWTyMtM8TqWSFRTEUjYOOf40T9WUlq5l59dMpYrJ+dqvCCRCKD1cQmbxVuqWFZSzbfPGsENU4eoBEQihIpAwuaxDzbROymOyyfleh1FRNpQEUhYrN++m9dXbuOaEweTqmsJi0QUFYGETKvP8fzCEkqr9nL3KyvplRTPzacM8zqWiLQT0j/NzGwa8CAQC/zZOXdfB/NcBdwDOGCZc+7aUGaS8Jm1rIzvv7Sc+FijudXx80vH0jct0etYItJOyIrAzGKBGcDZQCmw0MxmOec+aTPPCOCHwGedc1Vm1i9UeSS8fD7Hw+9uZHi/NEYN6EVjs4/pJwz2OpaIdCCUawRTgA3OuSIAM3sWuBj4pM08NwEznHNVAM65HSHMI2H01urtrNu+hweunsAlE3O8jiMiBxHKfQQ5QEmb+6WBaW2NBEaa2TwzWxDYlCTd3DtrtvP9l5YzpG8KF4wf6HUcETkEr3cWxwEjgNOBa4BHzaxP+5nM7GYzKzSzwoqKijBHlMMxa9lWvvJEIYPSk3nyy1OI09ARIhEvlL+lZUBem/u5gWltlQKznHPNzrlNwDr8xXAA59xM51yBc64gOzs7ZIHl6CwtqeaOF5YxJT+Tl795EvlZqV5HEpEghLIIFgIjzGyomSUA04FZ7eb5J/61AcwsC/+moqIQZpIQqapr4htPLyIrLZGHr5+ks4ZFupGQFYFzrgW4BXgDWA0875xbZWb3mtlFgdneAHaZ2SfAHOAO59yuUGWS0HDOcceLy9m5p5GHr5+kQ0RFupmQnkfgnJsNzG437cdtbjvge4Ev6aae+HAzb63ezt0XjGZ87qd28YhIhNOePDkqK8tq+MXsNZx1XD++8tl8r+OIyBHQoC9yROqbWvn1G2t5buEW+qYl8OsrjsfMvI4lIkdARSBH5A9z1vP4h5u4ZEIOt505gozUBK8jicgRUhFI0Jxz1Na34HA8+WEx540byO+unuB1LBE5SioCCUp9Uyt3vLiMf68oZ0S/NPY0tnDrGcO9jiUiXUA7i+WQ9ja1cM2jC/j3inLOHTuALZV7OX/8QI4d0NvraCLSBYJaIzCzl4HHgNecc77QRpJI4vM5bn9hGctKq5lx7STOGzeQ3Q3NJMTpbwiRniLY3+Y/AtcC683sPjMbFcJMEgGcc8xZu4PpMxcwe8U2fnTucZw3zj+AXK+keBLjdOawSE8R1BqBc+4t4C0zS8c/ONxbZlYCPAo87ZxrDmFGCbMdtQ384KXlzFlbwaD0JH560Ri+8JkhXscSkRAJemexmfUFrgduAJYAfwNOBr5IYLwg6RnuffUT5hft4n/PP44vnpRPvEYQFenRgt1H8A9gFPAUcKFzrjzw0HNmVhiqcBJ+22oaeH3lNr50Uj5f1fWFRaJCsGsEDznn5nT0gHOuoAvziMf+9lExrc7xhc/kex1FRMIk2HX+0W0vGGNmGWb2zRBlEo/s3NPI3z/awpnH9mNw3xSv44hImARbBDc556r33QlcY/im0EQSL+xtauHGJxZS19TCd84a6XUcEQmjYIsg1tqMKGZmsYAGl+khWlp93PL3Jawoq+EP10xibE6615FEJIyC3UfwOv4dw38K3P9aYJr0AHe/sop31uzg55eO5azR/b2OIyJhFmwR/AD/h/83Avf/A/w5JIkkrJaWVPPMx1v42mnDuO5EnSsgEo2CPaHMBzwc+JIe5IXCEpLiY7jlcxpATiRaBXsewQjgF8BoIGnfdOecDjTvZhqaW2ls8ZGeHE9Dcyuzlm3l3LED6ZUU73U0EfFIsJuGHgd+AvwO+BzwZTRyabf0jacX8f76nZw+qh+9k+PY3dDClQW5XscSEQ8F+2Ge7Jx7GzDnXLFz7h7g/NDFklBYs62WOWsrmDwkg9Xltby8uIxjslOZOrSv19FExEPBrhE0mlkM/tFHbwHKgLTQxZJQ+MsHm0iOj+VPN0ymT0oCexpbiDUjJkbXGhaJZsGuEXwbSAFuAybjH3zui6EKJV1vx+4G/rlkK1dMzqVPiv8UkLTEOJITNJy0SLQ75BpB4OSxq51ztwN78O8fkG7mwbfW43OOG08e6nUUEYkwh1wjcM614h9uWrqp9dt38+zCEq6fOoT8rFSv44hIhAl2H8ESM5sFvADU7ZvonHs5JKmkS/3mzbWkxMdy25kjvI4iIhEo2CJIAnYBZ7SZ5gAVQYTbsmsvb36ynW+dPpzMVA0PJSKfFuyZxdov0E39df5mYs24fqqGjxCRjgV7ZvHj+NcADuCc+0qXJ5IuUbG7kcLNlTxXWMK0sQMYkJ506CeJSFQKdtPQq21uJwGXAlu7Po50hcaWVi6ZMY+y6nrM0JFCInJQwW4aeqntfTN7BvggJInkqL20qIyy6nruv+p4Th2ZTVZaoteRRCSCBbtG0N4IoF9XBpGu0dzqY8acDUzI68OlE3Nocz0hEZEOBbuPYDcH7iPYhv8aBRJh/r28nLLqen52yRiVgIgEJdhNQ71CHUS6xnMLSxicmcLnRmmFTUSCE9RYQ2Z2qZmlt7nfx8wuCV0sORIllXuZX7SLKybnam1ARIIW7KBzP3HO1ey745yrxn99AokgLy4qxQwun6zrC4hI8IItgo7mO9IdzRICPp/jxUWlnDw8i5w+yV7HEZFuJNgiKDSz+83smMDX/cCiQz3JzKaZ2Voz22Bmdx5kvsvNzJlZQbDB5UALinZRVl3PFVobEJHDFGwR3Ao0Ac8BzwINwLcO9oTA8NUzgHPxX+v4GjMb3cF8vfBf7+Cj4GNLey8sKqVXUhyfHzPA6ygi0s0Ee9RQHdDpX/SdmAJscM4VAZjZs8DFwCft5vsZ8EvgjsN8fQmobWjmtZXlXD4pl6R4XWhGRA5PsEcN/cfM+rS5n2FmbxziaTlASZv7pYFpbV93EpDnnPv3Ib7/zWZWaGaFFRUVwUSOGi8uKuWSGfNoaPZxZUGe13FEpBsKdtNQVuBIIQCcc1Uc5ZnFgWsg3w/8z6Hmdc7NdM4VOOcKsrOzj+bb9ijlNfV8/8VlJMXF8tA1E5mQ1+fQTxIRaSfYI398ZjbYObcFwMzy6WA00nbKgLZ/ouYGpu3TCxgLvBs45n0AMMvMLnLOFQaZK6q9WFiKz8HD109iSF9deUxEjkywRXAX8IGZvQcYcApw8yGesxAYYWZD8RfAdODafQ8GzkvI2nffzN4FblcJBMfnczxXWMJJx/RVCYjIUQlq05Bz7nWgAFgLPIN/c079IZ7TAtwCvAGsBp53zq0ys3vN7KKjSi28s2YHpVX1TJ8y2OsoItLNBTvo3FfxH+KZCywFpgLzOfDSlZ/inJsNzG437cedzHt6MFkE3l27g1ufWUJ+3xTOGd3f6zgi0s0Fu7P428AJQLFz7nPARKD64E+RUKiqa+LrTy8iPyuV57/+GR0uKiJHLdgiaHDONQCYWaJzbg0wKnSxpDMvLS6lodnH/VcdT79euvykiBy9YHcWlwbOI/gn8B8zqwKKQxdLOuKc4+8fb2Hi4D4cN7C313FEpIcI9sziSwM37zGzOUA68HrIUkmHFhRVUlRRx6+vGO91FBHpQQ57BFHn3HuhCCIH19Dcyr2vfkJmagIXjB/kdRwR6UE0lHQ30NDcyk//tYrV5bU89sUCkhO0g1hEuo6KIMK9uWobt7+wjNqGFm46ZShnHqfDRUWka6kIItzv3lpPVloiM66bxMnDsw79BBGRwxTs4aPigTXballdXssXT8rnlBHZug6xiISEiiCC/XPJVmJjjAvGD/Q6ioj0YCqCCOXzOV5ZWsZpI7Ppm5bodRwR6cFUBBFqeVkN5TUNXHi81gZEJLRUBBFq7roKzOC0kUd1/R8RkUNSEUSouesqGDsonczUBK+jiEgPpyKIQLUNzSwpqebUkTpcVERCT0UQgT7csJNWn+PUEbo+s4iEnoogAr2zZgdpiXFMGpLhdRQRiQIqgghT29DMq8vLmTZ2APGx+vGISOjpkybCvLyolL1NrXzhM0O8jiIiUUJFEEGcczy1oJjj8/owPreP13FEJEqoCCLIirIaNlbUcd2UwV5HEZEooiKIIB9s2AnA547VSWQiEj4qgggyf+MuRvXvRXYvjS0kIuGjIogQjS2tLNxcyUnD+3odRUSijIogQiwurqah2cdJx+hsYhEJLxVBhJi/cScxBicOy/Q6iohEGRVBBHDOMXvlNiYOzqB3UrzXcUQkyqgIIsCCoko27NjD9BPyvI4iIlFIRRABnlqwmT4p8Vx4/CCvo4hIFFIReGxrdT1vrNrOVQV5JMXHeh1HRKKQisBjv3ljLbExxg1TNbaQiHhDReChxVuqeHlJGTedMpS8zBSv44hIlFIReOg3b6ylX69Evnn6cK+jiEgUUxF4pLymnvlFu7h+6hBSE+O8jiMiUUxF4JFXl5XjHFykI4VExGMqAo/MWraV8bnp5Geleh1FRKKcisADGyv2sKKsRmsDIhIRQloEZjbNzNaa2QYzu7ODx79nZp+Y2XIze9vMouIYyqfmFxMfa1w0QUUgIt4LWRGYWSwwAzgXGA1cY2aj2822BChwzo0HXgR+Fao8kaKmvpnnC0u4cPwg+vVK8jqOiEhI1wimABucc0XOuSbgWeDitjM45+Y45/YG7i4AckOYJyI8+/EW9ja18pWTh3odRUQECG0R5AAlbe6XBqZ15kbgtY4eMLObzazQzAorKiq6MGJ4VdY18ej7RUwdlsnYnHSv44iIABGys9jMrgcKgF939LhzbqZzrsA5V5CdnR3ecF3onlmrqKlv5icXjvE6iojIfqE8k6kMaDuucm5g2gHM7CzgLuA051xjCPN46l/LtjJr2Va+d/ZIjhvY2+s4IiL7hXKNYCEwwsyGmlkCMB2Y1XYGM5sI/Am4yDm3I4RZPLV++25+8NJyJg/J4BunH+N1HBGRA4SsCJxzLcAtwBvAauB559wqM7vXzC4KzPZrIA14wcyWmtmsTl6uW/vu80tJSYjjj9dNIj42IrbGiYjsF9JBbpxzs4HZ7ab9uM3ts0L5/SPBxoo9rCyr5ScXjqZ/bx0uKiKRR3+ehtgbq7YB8PkxAzxOIiLSMRVBiL2xajvH56YzqE+y11FERDqkIgihbTUNLCup5hytDYhIBFMRhNALhf7z6bRZSEQimYogRHbuaeRPc4s4e3R/hvdL8zqOiEinVAQh8tDb66lvbuXOc4/1OoqIyEGpCEJgaUk1Ty8o5roTB3NMttYGRCSyqQi6WENzK7e/sIwBvZO44/OjvI4jInJIump6F3t6QTEbduzhya9MoVdSvNdxREQOSWsEXWzftYhPG9l9R0kVkeiiIuhCJZV7WV5aw3njBnodRUQkaCqCLvTaynIAzlcRiEg3oiLoIs45Xl1ezricdPIyU7yOIyISNBVBF5kxZwPLS2u4sqDHX3ZZRHoYHTV0FJpbfdz32hpWltXw0aZKLp2Yww1Th3gdS0TksGiN4Cj86b2NPPbBJppafXzppHx+efl4zMzrWCIih0VrBEdo4eZKHnx7PReMH8gfrp3kdRwRkSOmIjhMlXVNfPXJhSzeUk12r0R+etEYryOJiBwVFcEh/Om9jSzZUs3dF45mQO8kbntmCSu31nL3BaO5dGIOmakJXkcUETkqKoKDaGn18ae5RVTWNfHeugoyUxMoq67nl5eP4+oTBnsdT0SkS6gIDqKwuIrKuiZ+dN6xbNpZx+6GFm48eahKQER6FFz2ThAAAAkCSURBVBXBQbyxahsJcTFcd+IQUhO1qESkZ9Lho51wzvHmqu2cOiJLJSAiPZqKoBPLSmsoq67nnNG63rCI9Gwqgk78bUExKQmxTBunIhCRnk1F0EbN3mZ+++Za1m7bzaxlW7lkYg69dXEZEenhtPG7jcc+KOL372zg4Xc30uJzXH+ixg0SkZ5PRRDQ2NLK3z7awsTBfajY3Uh+31RGD+rtdSwRkZBTEQD1Ta38c2kZu+qaeHD6RKYOy6TVOa9jiYiERdQVQUurjzXbdlNZ18TJw7P420fF3POvT2j1OYb3S+Ozw/tiZtG3YEQkakXV511tQzOXzJhHUUUdAONz01lRVsPJw7M4bWQ2J4/I0jDSIhJ1oqoI7nllFcW79nLfZeNo9jl++doaJub1YeYNBSQnxHodT0TEE1FTBP9eXs7LS8r49pkjmD7FP1bQxRMGkRQXS0KcjqIVkegVNUXQOzmOs0f355Yzhv93ms4REBGJniI4ZUQ2p4zI9jqGiEjE0TYREZEoF9IiMLNpZrbWzDaY2Z0dPJ5oZs8FHv/IzPJDmUdERD4tZEVgZrHADOBcYDRwjZmNbjfbjUCVc2448Dvgl6HKIyIiHQvlGsEUYINzrsg51wQ8C1zcbp6LgScDt18EzjQdyC8iElahLIIcoKTN/dLAtA7ncc61ADVA3/YvZGY3m1mhmRVWVFSEKK6ISHTqFjuLnXMznXMFzrmC7Gwd+SMi0pVCWQRlQF6b+7mBaR3OY2ZxQDqwK4SZRESknVAWwUJghJkNNbMEYDowq908s4AvBm5fAbzjnIb9FBEJJwvl566ZnQc8AMQCf3HO/dzM7gUKnXOzzCwJeAqYCFQC051zRYd4zQqg+AgjZQE7j/C5oRap2ZTr8CjX4YvUbD0t1xDnXIfb1kNaBJHGzAqdcwVe5+hIpGZTrsOjXIcvUrNFU65usbNYRERCR0UgIhLloq0IZnod4CAiNZtyHR7lOnyRmi1qckXVPgIREfm0aFsjEBGRdlQEIiJRLmqK4FBDYocxR56ZzTGzT8xslZl9OzD9HjMrM7Olga/zPMi22cxWBL5/YWBappn9x8zWB/7NCHOmUW2WyVIzqzWz73i1vMzsL2a2w8xWtpnW4TIyv4cC77nlZjYpzLl+bWZrAt/7H2bWJzA938zq2yy7R8Kcq9OfnZn9MLC81prZ50OV6yDZnmuTa7OZLQ1MD8syO8jnQ2jfY865Hv+F/4S2jcAwIAFYBoz2KMtAYFLgdi9gHf5huu8Bbvd4OW0GstpN+xVwZ+D2ncAvPf45bgOGeLW8gFOBScDKQy0j4DzgNcCAqcBHYc51DhAXuP3LNrny287nwfLq8GcX+D1YBiQCQwO/s7HhzNbu8d8CPw7nMjvI50NI32PRskYQzJDYYeGcK3fOLQ7c3g2s5tOjskaStkOFPwlc4mGWM4GNzrkjPbP8qDnn5uI/C76tzpbRxcBfnd8CoI+ZDQxXLufcm84/qi/AAvzjfYVVJ8urMxcDzzrnGp1zm4AN+H93w57NzAy4CngmVN+/k0ydfT6E9D0WLUUQzJDYYWf+K7JNBD4KTLolsHr3l3BvgglwwJtmtsjMbg5M6++cKw/c3gb09yDXPtM58BfT6+W1T2fLKJLed1/B/5fjPkPNbImZvWdmp3iQp6OfXSQtr1OA7c659W2mhXWZtft8COl7LFqKIOKYWRrwEvAd51wt8DBwDDABKMe/WhpuJzvnJuG/qty3zOzUtg86/7qoJ8cbm3/gwouAFwKTImF5fYqXy6gzZnYX0AL8LTCpHBjsnJsIfA/4u5n1DmOkiPzZtXMNB/7REdZl1sHnw36heI9FSxEEMyR22JhZPP4f8t+ccy8DOOe2O+danXM+4FFCuErcGedcWeDfHcA/Ahm271vVDPy7I9y5As4FFjvntgcyer682uhsGXn+vjOzLwEXANcFPkAIbHrZFbi9CP+2+JHhynSQn53nywv2D4l/GfDcvmnhXGYdfT4Q4vdYtBRBMENih0Vg2+NjwGrn3P1tprfdrncpsLL9c0OcK9XMeu27jX9H40oOHCr8i8Ar4czVxgF/oXm9vNrpbBnNAr4QOLJjKlDTZvU+5MxsGvB94CLn3N4207PNf01xzGwYMAI46Ki/XZyrs5/dLGC6mSWa2dBAro/DlauNs4A1zrnSfRPCtcw6+3wg1O+xUO8Fj5Qv/HvX1+Fv8rs8zHEy/tW65cDSwNd5+IfjXhGYPgsYGOZcw/AfsbEMWLVvGeG/dOjbwHrgLSDTg2WWiv+CReltpnmyvPCXUTnQjH977I2dLSP8R3LMCLznVgAFYc61Af/2433vs0cC814e+BkvBRYDF4Y5V6c/O+CuwPJaC5wb7p9lYPoTwNfbzRuWZXaQz4eQvsc0xISISJSLlk1DIiLSCRWBiEiUUxGIiEQ5FYGISJRTEYiIRDkVgUiAmbXagSOddtkotYHRK70810GkU3FeBxCJIPXOuQlehxAJN60RiBxCYFz6X5n/Wg0fm9nwwPR8M3snMHja22Y2ODC9v/nH/18W+Dop8FKxZvZoYJz5N80sOTD/bYHx55eb2bMe/TcliqkIRP4rud2moavbPFbjnBsH/AF4IDDt98CTzrnx+Ad0eygw/SHgPefc8fjHu18VmD4CmOGcGwNU4z9bFfzjy08MvM7XQ/WfE+mMziwWCTCzPc65tA6mbwbOcM4VBQYE2+ac62tmO/EPj9AcmF7unMsyswog1znX2OY18oH/OOdGBO7/AIh3zv2fmb0O7AH+CfzTObcnxP9VkQNojUAkOK6T24ejsc3tVv67j+58/OPFTAIWBka/FAkbFYFIcK5u8+/8wO0P8Y9kC3Ad8H7g9tvANwDMLNbM0jt7UTOLAfKcc3OAHwDpwKfWSkRCSX95iPxXsgUuVh7wunNu3yGkGWa2HP9f9dcEpt0KPG5mdwAVwJcD078NzDSzG/H/5f8N/KNcdiQWeDpQFgY85Jyr7rL/kUgQtI9A5BAC+wgKnHM7vc4iEgraNCQiEuW0RiAiEuW0RiAiEuVUBCIiUU5FICIS5VQEIiJRTkUgIhLl/j8R6Gg7uv5CbwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1rAgRpxYhjpB"
      },
      "source": [
        "### Generate new lyrics!\n",
        "\n",
        "It's finally time to generate some new lyrics from the trained model, and see what we get. To do so, we'll provide some \"seed text\", or an input sequence for the model to start with. We'll also decide just how long of an output sequence we want - this could essentially be infinite, as the input plus the previous output will be continuously fed in for a new output word (at least up to our max sequence length)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DC7zfcgviDTp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "92c0ef25-4974-436d-c6d0-406ad2abfb3b"
      },
      "source": [
        "seed_text = \"im feeling chills\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "im feeling chills and i eyes will you leave me girl i beg this feet come care care sleep had as eyes crazy scars touch front hand stuff stuff stuff am new as care as good as new give a lousy so as care care secrets walls stuff stuff stuff hate am new as dumb breeze question am new as new dimension crazy world world talk a wonderful as pain night stuff walls scars park night start stuff stuff am new as new dimension give yourself a break care care care care yourself as care as as good new dimension you leave had\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}